# [Python] Patent-Analysis

ğŸ—“ï¸ **Date**: 2024.11.21 ~ 2025.07.09

<br/>

ğŸ“Š **Objective**
 1. Collection of USPTO patent data using a web crawler.
 2. Propose new product idea discovery by applying NLP to patent abstract data.
<br/>

ğŸ§© **Table of Contents**
|Num|Content|
|----|-------|
|01|[Crawling Data](https://github.com/git-jihyunpark/Patent-Analysis/blob/main/1_crawling_patents_year.ipynb)|
|02|[Data Preprocessing](https://github.com/git-jihyunpark/Patent-Analysis/blob/main/2_data_preprocessing.ipynb)|
|03|[LDA](https://github.com/git-jihyunpark/Patent-Analysis/blob/main/3_LDA.ipynb)
|04|[NuNER](https://github.com/git-jihyunpark/Patent-Analysis/blob/main/4_NuNER.ipynb)|
<br/>


## ğŸ”· Project: New product idea discovery using LDA and NER


ğŸ“Œ **Introduction**
- Since morphology analysis is mainly conducted based on expert knowledge, it tends to be subjective or biased.
- To overcome this limitation, a data-driven methodology for constructing morphology analysis is proposed.
- Collected patent data related to `wearable devices` and constructed a morphological matrix using LDA and NuNER.
- Proposed new product ideas through morphology analysis.
<br/>


ğŸ“‚ **Dataset**
- USPTO Data Collection
  - Period: 2023.01.01 â€“ 2023.12.31
  - Condition: Patent abstracts containing the keyword `wearable device`
  - Collection Results: A total of 10,672
<br/>

### 1. Crawling Data

- Set data collection criteria on Google Patents, retrieve the data, and download the patent numbers as a CSV file.
![image](https://github.com/user-attachments/assets/553e41b0-6ad6-4e3c-9ad4-120f697d570e)
<br/><br/>


- Read a CSV file containing a list of patent numbers and crawl detailed information for each patent from Google Patents.<br/> 
```python
# íŠ¹í—ˆ ë²ˆí˜¸ì— ëŒ€í•œ í¬ë¡¤ë§
def crawling_patents(input_csv, output_csv):
    if not os.path.exists(input_csv):
        print(f"âŒ Input file not found: {input_csv}")
        return

    df = pd.read_csv(input_csv, skiprows=1)
    df['patent_number'] = df['id'].apply(lambda x: f"US{x.split('-')[1]}B2" if pd.notnull(x) else None)
    df = df[df['patent_number'].notnull()].reset_index(drop=True)

    results = []
    for pat in df['patent_number']:
        data = fetch_google_patent(pat)
        if data:
            results.append(data)
        time.sleep(1)  # polite crawling

    if results:
        df_out = pd.DataFrame(results)
        df_out.to_csv(output_csv, index=False, encoding='utf-8-sig')
        print(f"âœ… Data saved to {output_csv}")
    else:
        print("âš ï¸ No data collected.")
```
<br/><br/>


- Automatically collect patent information (title, abstract, filing date, grant date, etc.) from Google Patents based on a specific patent number. <br/>
```python
# Google Patents ì •ë³´ ìˆ˜ì§‘
def fetch_google_patent(patent_number):
    url = f'https://patents.google.com/patent/{patent_number}/en'
    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"[{patent_number}] âŒ Status Code: {response.status_code}")
        return None

    soup = BeautifulSoup(response.content, 'html.parser')

    try:
        title_tag = soup.find('meta', {'name': 'DC.title'})
        abstract_tag = soup.find('meta', {'name': 'DC.description'})
        filed_tag = soup.find('meta', {'scheme': 'dateFiled'})
        granted_tag = soup.find('meta', {'scheme': 'dateIssued'})

        title = title_tag['content'] if title_tag else None
        abstract = abstract_tag['content'] if abstract_tag else None
        date_filed = filed_tag['content'] if filed_tag else None
        date_granted = granted_tag['content'] if granted_tag else None

        if not title and not abstract:
            print(f"[{patent_number}] âŒ Empty content. Skipped.")
            return None

        return {
            'patent_number': patent_number,
            'title': title,
            'abstract': abstract,
            'date_filed': date_filed,
            'date_granted': date_granted
        }

    except Exception as e:
        print(f"[{patent_number}] âŒ Parsing Error: {e}")
        return None
```
<br/><br/>


- Save patent information (title, abstract, filing date, grant date, etc.) as a CSV file. <br/>
```python
crawling_patents(
    input_csv='Data/gp-search-20250611-235346.csv',
    output_csv='wearable_devices_patents_2023.csv'
)
```
<br/><br/>


### 2. Data Preprocessing
- After reading the CSV file, apply preprocessing to the `abstract` column. <br/>
  - Convert to lowercase, remove punctuation and numbers, perform tokenization, remove stopwords, and apply lemmatization.
```python
def preprocess_text(file_path, output_file):
    # CSV íŒŒì¼ ì½ê¸°
    df = pd.read_csv(file_path)

    # ë°ì´í„° í”„ë ˆì„ì˜ ì²« ëª‡ ì¤„ í™•ì¸
    print(f"Processing {file_path}")
    print(df.head())

    # ì „ì²˜ë¦¬ í•¨ìˆ˜
    def clean_text(text):
        # í…ìŠ¤íŠ¸ê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³  ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
        if not isinstance(text, str):
            text = ""

        # ì†Œë¬¸ìë¡œ ë³€í™˜
        text = text.lower()
        # êµ¬ë‘ì  ë° ìˆ«ì ì œê±°
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\d+', '', text)
        # í† í°í™”
        tokens = text.split()
        # ë¶ˆìš©ì–´ ì œê±°
        tokens = [word for word in tokens if word not in stopwords.words('english')]
        # í‘œì œì–´ ì¶”ì¶œ
        lemmatizer = WordNetLemmatizer()
        tokens = [lemmatizer.lemmatize(word) for word in tokens]
        return ' '.join(tokens)

    # 'abstract' ì—´ì— ì „ì²˜ë¦¬ ì ìš©
    if 'abstract' in df.columns:        
        df['cleaned_abstract'] = df['abstract'].apply(clean_text)
    else:
        print(f"'abstract' ì—´ì´ {file_path}ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ì„ CSV íŒŒì¼ë¡œ ì €ì¥
    df.to_csv(output_file, index=False, encoding='utf-8')
    print(f"Processed data saved to {output_file}")
    
    # ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
    return df
```
<br/><br/>

- Save the preprocessing results as a CSV file. <br/>
```python
df_2023 = preprocess_text('Data/wearable_devices_patents_2023.csv', 'Data/wearable_devices_processed_2023.csv')
```
<br/><br/>



### 3. LDA
- Extracted topics from the data using the LDA model. <br/>
```python
def lda_modeling(cleaned_abstracts):
    # TF-IDFë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„° ë²¡í„°í™”
    vectorizer = TfidfVectorizer(max_df=0.75, min_df=5, stop_words='english')
    X = vectorizer.fit_transform(cleaned_abstracts)

    # ë°ì´í„° ë²¡í„°í™”ëœ ê²ƒì„ gensimì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜
    corpus = Sparse2Corpus(X, documents_columns=False)
    id2word = {i: token for i, token in enumerate(vectorizer.get_feature_names_out())}

    # Perplexityì™€ Coherence ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    perplexity_scores = []
    coherence_scores = []
    topic_range = range(5, 31, 1)  # 5ì—ì„œ 30ê¹Œì§€ 1ë‹¨ìœ„ë¡œ í† í”½ ìˆ˜ ì¡°ì •

    # ê° í† í”½ ìˆ˜ì— ë”°ë¥¸ LDA ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    for num_topics in topic_range:
        # sklearn LDA ëª¨ë¸ í•™ìŠµ
        lda_model = LatentDirichletAllocation(n_components=num_topics, learning_decay=0.7, random_state=42)
        lda_model.fit(X)

        # Perplexity ê³„ì‚° (scikit-learnì˜ LDA ëª¨ë¸ë¡œ ê³„ì‚°)
        perplexity = lda_model.perplexity(X)
        perplexity_scores.append(perplexity)

        # Gensimì˜ LDA ëª¨ë¸ì„ ì‚¬ìš©í•´ Coherence ê³„ì‚°
        gensim_model = gensim.models.LdaModel(
            corpus=corpus,
            id2word=id2word,
            num_topics=num_topics,
            passes=10,
            iterations=50,
            random_state=42,
            alpha='auto'
        )

        # Coherence ê³„ì‚°
        coherence_model = CoherenceModel(
            model=gensim_model,
            texts=[doc.split() for doc in cleaned_abstracts],  # 'cleaned_abstracts'ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            dictionary=Dictionary.from_corpus(corpus, id2word),
            coherence='c_v'
        )
        coherence = coherence_model.get_coherence()
        coherence_scores.append(coherence)

    # ê²°ê³¼ ë°˜í™˜
    return perplexity_scores, coherence_scores, X, vectorizer
```
<br/><br/>

- Calculated Perplexity and Coherence to determine the optimal number of LDA topics and visualized the results in a graph. <br/>
```python
def lda_plot(perplexity_scores, coherence_scores):
    # í† í”½ ë²”ìœ„ (5ì—ì„œ 30ê¹Œì§€ 1ë‹¨ìœ„ë¡œ í† í”½ ìˆ˜ ì¡°ì •)
    topic_range = range(5, 31, 1)
    
    # ê·¸ë˜í”„ í¬ê¸° ì„¤ì •
    plt.figure(figsize=(12, 5))
    
    # Perplexity ê·¸ë˜í”„
    plt.subplot(1, 2, 1)
    plt.plot(list(topic_range), perplexity_scores, marker='o', color='blue')
    plt.title('Perplexity of LDA Models')
    plt.xlabel('Number of Topics')
    plt.ylabel('Perplexity')
    for i, txt in enumerate(perplexity_scores):
        plt.annotate("", (topic_range[i], perplexity_scores[i]), textcoords="offset points", xytext=(0,10), ha='center')
        # Uncomment to show Perplexity scores as text on graph
        # plt.annotate(f"{txt:.1f}", (topic_range[i], perplexity_scores[i]), textcoords="offset points", xytext=(0,10), ha='center')

    # Coherence ê·¸ë˜í”„
    plt.subplot(1, 2, 2)
    plt.plot(list(topic_range), coherence_scores, marker='o', color='red')
    plt.title('Coherence Score by Number of Topics')
    plt.xlabel('Number of Topics')
    plt.ylabel('Coherence Score')
    for i, txt in enumerate(coherence_scores):
        plt.annotate("", (topic_range[i], coherence_scores[i]), textcoords="offset points", xytext=(0,10), ha='center')
        # Uncomment to show Coherence scores as text on graph
        # plt.annotate(f"{txt:.2f}", (topic_range[i], coherence_scores[i]), textcoords="offset points", xytext=(0,10), ha='center')

    # ê·¸ë˜í”„ ë ˆì´ì•„ì›ƒ ì¡°ì • ë° ì¶œë ¥
    plt.tight_layout()
    plt.show()
```
<br/><br/>

- The number of LDA topics was determined as 8, 12, and 16 based on the following criteria:
  - The number of components in wearable devices
  - The complexity of the model
  - The trade-off relationship between perplexity and coherence
<img width="1189" height="490" alt="LDA" src="https://github.com/user-attachments/assets/f9cdff22-af59-4f7e-883f-06b7c6d5f916" />
<br/><br/>


- Extracted top keywords for each number of topics.
```python
# í† í”½ ìˆ˜
topics_8 = 8

lda = LatentDirichletAllocation(n_components=topics_8, random_state=42)
lda.fit(X_2023)

topics = display_topics(lda, vectorizer_2023.get_feature_names_out(), num_top_words)

# ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ (topic, keywords ë‘ ì»¬ëŸ¼)
df_topics_8 = pd.DataFrame(list(topics.items()), columns=['topic', 'keywords'])

# ê²°ê³¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
output_file = 'Data/wearable_devices_2023_lda_topic_8.csv'
df_topics_8.to_csv(output_file, index=False)

# ê²°ê³¼ ì¶œë ¥
for topic_num, words in topics.items():
    print(f"{topic_num}: {words}\n")
```
<br/><br/>


- Reviewed the keywords and defined the `dimensions` of the morphological matrix based on expert knowledge.
```python
# ê° í† í”½ë³„ dimension ì •ì˜(ì „ë¬¸ê°€ ì§€ì‹ ê¸°ë°˜)
labels_8 = {
    'Topic 0': 'Data Processing',
    'Topic 1': 'Wireless Communication',
    'Topic 2': 'User Interface',
    'Topic 3': 'Audio System',
    'Topic 4': 'Optical Technology',
    'Topic 5': 'Materials',
    'Topic 6': 'Network System',
    'Topic 7': 'Sensors'
}

# CSV íŒŒì¼ ì½ê¸°
df_2023_topic_8 = pd.read_csv('Data/wearable_devices_2023_lda_topic_8.csv')

# label ì¶”ê°€
df_2023_topic_8['label'] = df_2023_topic_8['topic'].map(labels_8)

# ì»¬ëŸ¼ ìˆœì„œ 'label', 'topic', 'keywords' ìˆœìœ¼ë¡œ ë³€ê²½
df_2023_topic_8 = df_2023_topic_8[['label', 'topic', 'keywords']]

# ê²°ê³¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
output_file = 'Data/wearable_devices_2023_lda_topic_8_with_labels.csv'
df_2023_topic_8.to_csv(output_file, index=False)

# ê²°ê³¼ í™•ì¸
df_2023_topic_8
```
<br/><br/>




### 4. NuNER

```python

```
<br/><br/>



```python

```
<br/><br/>





---


ğŸ’– **Lesson & Learn**
1. Improvement of data collection and NLP 
   > USPTO patent data <br/>
   > LDA, NuNER
2. Discovery of new product ideas  
   > Discovery of new product ideas based on morphological analysis






